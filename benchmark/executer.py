#!/usr/bin/env python3.5
"""
Runs benchmarks for testing gsp algorithms in a distributed fashion.

When run it connects to the distributed queue manager running on andorra, submits jobs and waits for the result.
"""

import logging
import multiprocessing
import os
import queue
import subprocess
from collections import namedtuple
from itertools import chain
from multiprocessing.managers import BaseManager
from random import randrange

import configargparse as argparse
from peewee import DoesNotExist

from benchmark import model, config as remote_config
from geometry import Point, TriangulatedPolygon
from geometry.polygons import concave_triangle, convex_sleeve_polygon, pathological_03, star_polygon
from gsp import delaunay_shortest_path, lee_preparata_shortest_path, makestep_shortest_path, trapezoid_shortest_path

config = dict(
    min_n=5,
    max_n=10,
    step=1,
    polygons=10,
    digits=4,
    bounding_box=10,
    file='benchmark-data',
    without=[],
    min_runs=3,
    max_runs=10,
    max_time=1.0,
    type='random',
    type_options=('random', 'convex_sleeve', 'star', 'concave_triangle', 'pathological_03'),
)

algorithms = dict(delaunay=delaunay_shortest_path, makestep=makestep_shortest_path, trapezoid=trapezoid_shortest_path,
                  lee_preparata=lee_preparata_shortest_path)


class QueueManager(BaseManager):
    """Dummy queue manager."""

    pass


def uniq(seq, idfun=None):
    """Make a list unique."""
    # order preserving
    if idfun is None:
        def idfun(x):
            return x
    seen = {}
    result = []
    for item in seq:
        marker = idfun(item)
        if marker in seen:
            continue
        seen[marker] = 1
        result.append(item)
    return result


def reduce(function, iterable):
    """Reduce a list with a function but try to split in the middle."""
    if not isinstance(iterable, list):
        iterable = list(iterable)

    if len(iterable) == 0:
        return None

    if len(iterable) == 1:
        return iterable[0]

    if len(iterable) % 2 == 0:
        left = reduce(function, iterable[:len(iterable) // 2])
        right = reduce(function, iterable[len(iterable) // 2:])
        return function(left, right)

    left = reduce(function, iterable[:len(iterable) // 2])
    right = reduce(function, iterable[len(iterable) // 2:-1])
    return function(function(left, right), iterable[-1])


def create_random_polygons(vertices, number):
    """
    Return random polygons generated by a java program.

    :param vertices: the number of vertices each polygon should have
    :param number:   how many polygons should be generated
    :return: a list of random polygons
    """
    dir = os.path.join(os.path.dirname(os.path.dirname(os.path.dirname(os.path.realpath(__file__)))), 'polygons-swp')

    # Run polygon creator
    process = subprocess.run(
        ['./run.sh', '--boundingbox', str(config['bounding_box']), '--number', str(number), '--points', str(vertices),
         '--no-statistics', '--no-header'],
        cwd=dir, stderr=subprocess.DEVNULL, stdout=subprocess.PIPE,
        universal_newlines=True
    )

    # Extract each polygon from its output line
    polygons = []
    for line in process.stdout.strip().split('\n'):
        points = [Point(round(float(x), config['digits']), round(float(y), config['digits'])) for x, y in
                  [point.split(' ') for point in line.split(':')]]
        polygons.append(TriangulatedPolygon(points))

    return polygons


def get_polygons(type_, vertices, number):
    """Return (a) polygon(s) of a specific type. If 'random' is selected multiple polygons are returned."""
    if type_ == 'random':
        return create_random_polygons(vertices, number)
    elif type_ == 'convex_sleeve':
        return [convex_sleeve_polygon(vertices, height=config['bounding_box'], width=config['bounding_box'],
                                      triangulated=True, round=config['digits'])]
    elif type_ == 'star':
        return [star_polygon(vertices, big_radius=config['bounding_box'] / 2, small_radius=config['bounding_box'] / 4,
                             triangulated=True, round=config['digits'])]
    elif type_ == 'concave_triangle':
        return [
            concave_triangle(vertices, height=config['bounding_box'], width=config['bounding_box'], triangulated=True,
                             round=config['digits'])]
    elif type_ == 'pathological_03':
        return [
            pathological_03(vertices, size=config['bounding_box'], triangulated=True,
                            round=config['digits'])]

    return []


def main():
    """Run benchmarks for randomly generated polygons with (some of) the gsp algorithms."""
    if config['run_missing']:
        config['run_missing'] = list(chain(*[m.split(',') for m in config['run_missing']]))

    def n_values():
        return range(config['min_n'], config['max_n'] + 1, config['step'])

    algorithm_list = list(algorithms.keys())

    def submit(polygon, polygon_id, s_id, t_id, s, t, percentage, algorithms=None):
        args = (polygon, polygon_id, s_id, t_id, s, t, algorithms or algorithm_list)
        job_queue.put(args)
        all_jobs.add((polygon_id, s_id, t_id))
        print('Submitted {size}: ({polygon}, {s}, {t})\t{percentage}% / {polygon_percentage}%'
              .format(size=len(polygon), polygon=polygon_id, s=s_id, t=t_id, percentage=int(percentage * 100),
                      polygon_percentage=int(no_polygons / all_polygons * 100)))
        try:
            while len(all_jobs) > 0:
                result = result_queue.get(block=len(all_jobs) > 200)
                try:
                    print('Got result for', result)
                    assert result in all_jobs
                except Exception as e:
                    print('Result threw exception', flush=True)
                    print('EXCEPTION', flush=True)
                    print(e, flush=True)
                    print('RESULT', flush=True)
                    print(result, flush=True)
                else:
                    all_jobs.remove(result)
        except queue.Empty:
            pass

    m = QueueManager(address=(remote_config.host, remote_config.port),
                     authkey=remote_config.authkey)
    m.connect()

    job_queue = m.get_job_queue()
    result_queue = m.get_result_queue()

    m_polygon_type, created = model.PolygonType.get_or_create(name=config['type'])
    if created:
        logging.debug('Created model type "%s"', m_polygon_type)
    else:
        logging.debug('Got model type "%s"', m_polygon_type)

    all_jobs = set()

    no_polygons = 0
    all_polygons = config['polygons'] * len(n_values())

    def create_polygons():
        logging.info('Creating new polygons')
        for n in n_values():
            for polygon in get_polygons(config['type'], n, config['polygons']):
                m_polygon = model.Polygon.create(type=m_polygon_type, size=len(polygon))
                logging.debug('Created polygon "%s"', m_polygon)
                for index, point in enumerate(polygon.points):
                    m_point, _ = model.Point.get_or_create(x=point.x, y=point.y)
                    model.PolygonPoint.create(index=index, polygon=m_polygon, point=m_point)
                yield m_polygon, polygon

    def load_polygons():
        logging.info('Loading existing polygons')
        for i in config['run_missing']:
            try:
                m_polygon = model.Polygon.get(id=int(i))
            except (model.Polygon.DoesNotExist, ValueError):
                pass
            else:
                polygon = TriangulatedPolygon(m_polygon.as_geometry())
                yield m_polygon, polygon

    def create_or_load_polygons():
        global all_polygons
        if config['run_missing']:
            all_polygons = len(config['run_missing'])
            return load_polygons()
        return create_polygons()

    for m_polygon, polygon in create_or_load_polygons():
        no_polygons += 1
        logging.debug('Got polygon no. %d', no_polygons)

        if config['run_missing'] and config['pair_subset'] == 0:

            logging.debug('Rerun missing instances')

            run_dict = dict()

            for run in m_polygon.runs:
                run_algorithms = set(algorithm_list)
                for instance in run.instances:
                    run_algorithms.discard(instance.algorithm.name)
                m_s, m_t = run.s, run.t

                if (m_s, m_t) in run_dict:
                    run_dict[(m_s, m_t)].intersection_update(run_algorithms)
                else:
                    run_dict[(m_s, m_t)] = run_algorithms

            all_runs = len(run_dict)

            for ix, ((m_s, m_t), run_algorithms) in enumerate(run_dict.items()):
                if not run_algorithms:
                    continue
                submit(polygon, m_polygon.id, m_s.id, m_t.id, m_s.point.as_geometry(), m_t.point.as_geometry(),
                       (ix + 1) / all_runs, run_algorithms)

            continue

        if not config['run_missing'] and config['pathological']:
            logging.debug('Run only pathological instances')
            if config['type'] == 'pathological_03':

                x1, y1 = polygon.point(-1).tuple()
                x2, _ = polygon.point(-2).tuple()
                x3, y3 = polygon.point(-3).tuple()
                s = Point((x1 + x2) / 2, y1 + (y3 - y1) / (x3 - x2) * (x1 - x2) / 4).round(config['digits'])
                m_point, _ = model.Point.get_or_create(x=s.x, y=s.y)
                m_s, created = model.PolygonPoint.get_or_create(polygon=m_polygon, point=m_point, is_vertex=False,
                                                                defaults=dict(index=-1))

                x4, y4 = polygon.point(-4).tuple()
                x5, y5 = polygon.point(-5).tuple()
                t = Point((2 * x4 + x5) / 3, (y4 + y5) / 2).round(config['digits'])
                m_point, _ = model.Point.get_or_create(x=t.x, y=t.y)
                m_t, created = model.PolygonPoint.get_or_create(polygon=m_polygon, point=m_point, is_vertex=False,
                                                                defaults=dict(index=-1))

                submit(polygon, m_polygon.id, m_s.id, m_t.id, s, t, 1)

            continue

        logging.debug('Collect all points')

        points_inside = list(map(lambda i: (i, polygon.point_inside_at(i)), range(len(polygon))))
        query, query_data = model.Point.insert_many((dict(x=p.x, y=p.y) for _, p in points_inside)).sql()
        query += ' ON CONFLICT DO NOTHING'
        with model.db.atomic():
            model.db.execute_sql(query, query_data)

        points_inside = uniq(points_inside, lambda entry: entry[1].tuple())
        points_inside = sorted(points_inside, key=lambda entry: entry[1].tuple())
        constraints = [(model.Point.x == p.x) & (model.Point.y == p.y) for _, p in points_inside]
        m_points = (model.Point
                    .select()
                    .where(reduce(lambda p1, p2: p1 | p2, constraints))
                    .order_by(model.Point.x, model.Point.y))

        assert len(points_inside) == m_points.count()

        logging.debug('Collect all polygon points')

        m_polygon_point_data = []
        for index, m_point in enumerate(m_points):
            m_polygon_point_data.append(dict(polygon=m_polygon, point=m_point, is_vertex=False,
                                             index=points_inside[index][0]))

        query, query_data = model.PolygonPoint.insert_many(m_polygon_point_data).sql()
        query += ' ON CONFLICT DO NOTHING'
        with model.db.atomic():
            model.db.execute_sql(query, query_data)

        m_polygon_points = (model.PolygonPoint
                            .select(model.PolygonPoint, model.Point)
                            .join(model.Point)
                            .where(model.PolygonPoint.polygon == m_polygon,
                                   ~model.PolygonPoint.is_vertex)
                            .order_by(model.PolygonPoint.index))

        points = []
        for m_polygon_point in m_polygon_points:
            points.append((m_polygon_point, m_polygon_point.point.as_geometry()))

        no_points = 0
        if config['pair_subset'] > 0:
            logging.debug('Running a subset of %d point pairs', config['pair_subset'])
            while no_points < config['pair_subset']:
                pair = randrange(len(points) * len(points))
                m_s, s = points[pair // len(points)]
                m_t, t = points[pair % len(points)]
                try:
                    model.Run.get(polygon=m_polygon, s=m_s, t=m_t)
                except DoesNotExist:
                    no_points += 1
                    submit(polygon, m_polygon.id, m_s.id, m_t.id, s, t, no_points / config['pair_subset'])
        else:
            logging.debug('Running all point pairs')
            for m_s, s in points:
                for m_t, t in points:
                    no_points += 1
                    submit(polygon, m_polygon.id, m_s.id, m_t.id, s, t, no_points / len(points) / len(points))

    try:
        while len(all_jobs) > 0:
            result = result_queue.get(block=True)
            try:
                print('Got result for', result)
                assert result in all_jobs
            except Exception as e:
                print('Result threw exception', flush=True)
                print('EXCEPTION', flush=True)
                print(e, flush=True)
                print('RESULT', flush=True)
                print(result, flush=True)
            else:
                all_jobs.remove(result)
    except queue.Empty:
        pass


if __name__ == '__main__':

    parser = argparse.ArgumentParser(description='Benchmark GSP algorithms with automatic load balancing.',
                                     formatter_class=argparse.ArgumentDefaultsHelpFormatter)
    parser.add_argument('-c', '--config', is_config_file=True, help='config file path')
    parser.add_argument('--min-n', type=int, help='what is the minimal n value', default=config['min_n'])
    parser.add_argument('--max-n', type=int, help='maximum number of vertices', default=config['max_n'])
    parser.add_argument('--step', type=int, help='steps to take increasing n', default=config['step'])
    parser.add_argument('--type', type=str, help='which type of polygons to test', default=config['type'],
                        choices=config['type_options'])
    parser.add_argument('--polygons', type=int, help='number of polygons per n', default=config['polygons'])
    parser.add_argument('--digits', type=int, help='number of digits to round to', default=config['digits'])
    parser.add_argument('--bounding_box', type=int, help='size of the bounding box',
                        default=config['bounding_box'])
    parser.add_argument('--file', type=str, help='file prefix', default=config['file'])
    parser.add_argument('--without', action='append', help='skip this algorithm', default=config['without'])
    parser.add_argument('--min-runs', type=int, help='number each test should be run at least',
                        default=config['min_runs'])
    parser.add_argument('--max-runs', type=int, help='number each test should be run at most',
                        default=config['max_runs'])
    parser.add_argument('--max-time', type=int, help='maximum time each test should be run',
                        default=config['max_time'])

    parser.add_argument('--run-missing', action='append', help='ids of polygons where missing instances should be run',
                        default=[])
    parser.add_argument('--pair-subset', type=int, default=0, help='take a random subset with this size')

    parser.add_argument('--pathological', action='store_true', help='test only pathological case(s)')

    parser.add_argument('-v', '--verbose', action='store_true', help='be more verbose')
    parser.add_argument('-q', '--quiet', action='store_true', help='be quiet')

    args = parser.parse_args()

    for algorithm in args.without:
        try:
            del algorithms[algorithm]
        except KeyError:
            pass

    if args.verbose:
        logging.basicConfig(level=logging.DEBUG)
    elif args.quiet:
        logging.basicConfig(level=logging.WARNING)
    else:
        logging.basicConfig(level=logging.INFO)

    config = vars(args)

    multiprocessing.log_to_stderr(logging.DEBUG)

    QueueManager.register('get_job_queue')
    QueueManager.register('get_result_queue')

    main()
